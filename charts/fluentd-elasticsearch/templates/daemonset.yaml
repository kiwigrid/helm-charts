apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: {{ include "fluentd-elasticsearch.fullname" . }}
  labels:
{{ include "fluentd-elasticsearch.labels" . | indent 4 }}
    {{- if semverCompare "> 1.6" .Capabilities.KubeVersion.GitVersion }}
    kubernetes.io/cluster-service: "true"
    {{- end }}
    addonmanager.kubernetes.io/mode: Reconcile
{{- if .Values.annotations }}
  annotations:
{{ toYaml .Values.annotations | indent 4 }}
{{- end }}
spec:
  updateStrategy:
{{ toYaml .Values.updateStrategy | indent 4 }}
  selector:
    matchLabels:
      app.kubernetes.io/name: {{ include "fluentd-elasticsearch.name" . }}
      app.kubernetes.io/instance: {{ .Release.Name }}
  template:
    metadata:
      labels:
{{ include "fluentd-elasticsearch.labels" . | indent 8 }}
        {{- if semverCompare "> 1.6" .Capabilities.KubeVersion.GitVersion }}
        kubernetes.io/cluster-service: "true"
        {{- end }}
      annotations:
        {{- if semverCompare "< 1.13" .Capabilities.KubeVersion.GitVersion }}
        # This annotation ensures that fluentd does not get evicted if the node
        # supports critical pod annotation based priority scheme.
        # Note that this does not guarantee admission on the nodes (#40573).
        # NB! this annotation is deprecated as of version 1.13 and will be removed in 1.14.
        # ref: https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/
        scheduler.alpha.kubernetes.io/critical-pod: ''
        {{- end }}
        checksum/config: {{ include (print $.Template.BasePath "/configmaps.yaml") . | sha256sum }}
{{- if .Values.podAnnotations }}
{{ toYaml .Values.podAnnotations | indent 8 }}
{{- end }}
    spec:
      serviceAccountName: {{ include "fluentd-elasticsearch.fullname" . }}
      {{- if .Values.priorityClassName }}
      priorityClassName: {{ .Values.priorityClassName | quote }}
      {{- end }}
      {{- if .Values.image.pullSecrets }}
      imagePullSecrets:
      {{- range .Values.image.pullSecrets }}
        - name: {{ . }}
      {{- end }}
      {{- end }}
      containers:
      - name: {{ include "fluentd-elasticsearch.fullname" . }}
        image:  "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
        imagePullPolicy: {{ .Values.image.pullPolicy | quote }}
        {{- if .Values.additionalPlugins }}
        command:
          - /bin/bash
        args:
          - '-c'
          - >-
            {{- range .Values.additionalPlugins }}
            gem install {{ .name }} --version {{ .version }} &&
            {{- end }}
            /run.sh
        {{- end }}
        env:
        - name: FLUENTD_ARGS
          value: {{ .Values.fluentdArgs | quote }}
        - name: OUTPUT_HOST
          {{- if .Values.awsSigningSidecar.enabled }}
          value: 'localhost'
          {{- else }}
          value: {{ .Values.elasticsearch.host | quote }}
          {{- end }}
        - name: OUTPUT_PORT
          {{- if .Values.awsSigningSidecar.enabled }}
          value: '8080'
          {{- else }}
          value: {{ .Values.elasticsearch.port | quote }}
          {{- end }}
        - name: OUTPUT_PATH
          value: {{ .Values.elasticsearch.path | quote }}
{{- if .Values.elasticsearch.auth.enabled }}
        - name: OUTPUT_USER
          value: {{ .Values.elasticsearch.auth.user | quote }}
{{- if .Values.elasticsearch.auth.password }}
        - name: OUTPUT_PASSWORD
          value: {{ .Values.elasticsearch.auth.password | quote }}
{{- end }}
{{- end }}
        - name: LOGSTASH_PREFIX
          value: {{ .Values.elasticsearch.logstashPrefix | quote }}
        - name: OUTPUT_SCHEME
          {{- if .Values.awsSigningSidecar.enabled }}
          value: 'http'
          {{- else }}
          value: {{ .Values.elasticsearch.scheme | quote }}
          {{- end }}
        - name: OUTPUT_SSL_VERIFY
          value: {{ .Values.elasticsearch.sslVerify | quote }}
        - name: OUTPUT_SSL_VERSION
          value: {{ .Values.elasticsearch.sslVersion | quote }}
        - name: OUTPUT_TYPE_NAME
          value: {{ .Values.elasticsearch.typeName | quote }}
        - name: OUTPUT_BUFFER_CHUNK_LIMIT
          value: {{ .Values.elasticsearch.bufferChunkLimit | quote }}
        - name: OUTPUT_BUFFER_QUEUE_LIMIT
          value: {{ .Values.elasticsearch.bufferQueueLimit | quote }}
        - name: OUTPUT_LOG_LEVEL
          value: {{ .Values.elasticsearch.logLevel | quote }}
        {{- if .Values.env }}
        {{- range $key, $value := .Values.env }}
        - name: {{ $key }}
          value: {{ $value | quote }}
        {{- end }}
        {{- end }}
        {{- if .Values.secret }}
        {{- range $key, $value := .Values.secret }}
        - name: {{ .name }}
          valueFrom:
            secretKeyRef:
              name: {{ $value.secret_name }}
              key: {{ $value.secret_key | quote }}
        {{- end }}
        {{- end }}
        - name: K8S_NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        resources:
{{ toYaml .Values.resources | indent 10 }}
        volumeMounts:
        - name: varlog
          mountPath: {{ .Values.hostLogDir.varLog }}
        - name: varlibdockercontainers
          mountPath: {{ .Values.hostLogDir.dockerContainers }}
          readOnly: true
        - name: libsystemddir
          mountPath: {{ .Values.hostLogDir.libSystemdDir }}
          readOnly: true
        - name: config-volume
          mountPath: /etc/fluent/config.d
{{- if .Values.extraVolumeMounts }}
{{ toYaml .Values.extraVolumeMounts | indent 8 }}
{{- end }}
      {{- if .Values.livenessProbe.enabled }}  #pointing to fluentd Dockerfile
        # Liveness probe is aimed to help in situarions where fluentd
        # silently hangs for no apparent reasons until manual restart.
        # The idea of this probe is that if fluentd is not queueing or
        # flushing chunks for 5 minutes, something is not right. If
        # you want to change the fluentd configuration, reducing amount of
        # logs fluentd collects, consider changing the threshold or turning
        # liveness probe off completely.
        livenessProbe:
          initialDelaySeconds: {{ .Values.livenessProbe.initialDelaySeconds }}
          periodSeconds: {{ .Values.livenessProbe.periodSeconds }}
          exec:
            command:
{{ toYaml .Values.livenessProbe.command | indent 12 }}
{{- end }}
        ports:
{{- range $port := .Values.service.ports }}
          - name: {{ $port.name }}
            containerPort: {{ $port.port }}
{{- if $port.protocol }}
            protocol: {{ $port.protocol }}
{{- end }}
{{- end }}
      {{- if .Values.awsSigningSidecar.enabled }}
      - name: {{ include "fluentd-elasticsearch.fullname" . }}-aws-es-proxy
        image: {{ .Values.awsSigningSidecar.image.repository }}:{{ .Values.awsSigningSidecar.image.tag }}
        imagePullPolicy: {{ .Values.image.pullPolicy | quote }}
        args: ["-endpoint", "{{ .Values.elasticsearch.scheme }}://{{ .Values.elasticsearch.host }}:{{ .Values.elasticsearch.port }}",
               "-listen",   "127.0.0.1:8080"]
        env:
        - name: PORT_NUM
          value: "{{ .Values.elasticsearch.port }}"
      {{- end }}
      terminationGracePeriodSeconds: 30
      volumes:
      - name: varlog
        hostPath:
          path: {{ .Values.hostLogDir.varLog }}
      - name: varlibdockercontainers
        hostPath:
          path: {{ .Values.hostLogDir.dockerContainers }}
      # It is needed to copy systemd library to decompress journals
      - name: libsystemddir
        hostPath:
          path: {{ .Values.hostLogDir.libSystemdDir }}
      - name: config-volume
        configMap:
          name: {{ include "fluentd-elasticsearch.fullname" . }}
{{- if .Values.extraVolumes }}
{{ toYaml .Values.extraVolumes | indent 6 }}
{{- end }}
{{- if .Values.affinity }}
      affinity:
{{ toYaml .Values.affinity | indent 8 }}
{{- end }}
{{- if .Values.nodeSelector }}
      nodeSelector:
{{ toYaml .Values.nodeSelector | indent 8 }}
{{- end }}
{{- if .Values.tolerations }}
      tolerations:
{{ toYaml .Values.tolerations | indent 6 }}
{{- end }}
